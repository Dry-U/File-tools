# 智能文件检索与问答系统

## 1. 项目概述

### 1.1 项目背景
随着本地文件数量快速增长和隐私保护需求增强，用户需要一款完全离线运行的智能文件检索工具。本项目旨在开发一个基于Python的智能文件助手，结合快速文件索引技术和本地/在线大语言模型，实现高效的文件搜索、内容问答和个性化推荐功能。

### 1.2 核心目标
- 实现5秒内响应的高效文件搜索
- 支持多种文档格式的内容提取和索引
- 提供基于本地LLaMA模型或在线API的智能问答功能
- 集成搜索历史与个性化推荐系统
- 确保全部数据处理在本地完成，保护用户隐私

## 2. 技术栈确定

### 2.1 核心框架
- **编程语言**: Python 3.8+
- **GUI框架**: PyQt5 (跨平台桌面应用)
- **文件监控**: Watchdog (跨平台文件系统事件监控)
- **索引引擎**: Whoosh (纯Python全文搜索引擎)
- **向量搜索**: FAISS (Facebook相似性搜索库)

### 2.2 模型集成
- **本地推理**: LLaMA.cpp (通过WSL运行)
- **API备用**: OpenAI GPT或DeepSeek API
- **嵌入模型**: SentenceTransformers (all-MiniLM-L6-v2)
- **模型格式**: GGUF (量化模型格式)

### 2.3 文档处理
- **PDF**: PyPDF2 和 PDFMiner
- **Word**: python-docx
- **Excel**: openpyxl
- **文本**: 内置处理
- **Markdown**: 内置处理
- **其他格式**: textract (作为后备)

### 2.4 数据存储
- **索引存储**: Whoosh索引目录
- **元数据**: SQLite数据库
- **配置管理**: YAML配置文件
- **缓存管理**: 磁盘缓存 + SQLite

## 3. 系统架构

### 3.1 总体架构
系统采用模块化设计，主要包含以下组件：
1. **用户界面层**: PyQt5实现的GUI应用
2. **业务逻辑层**: 搜索、索引、推荐等核心功能
3. **数据处理层**: 文档解析、向量化、模型推理
4. **数据存储层**: 索引文件、数据库、配置文件

### 3.2 核心模块
1. **文件监控模块**: 使用Watchdog监控文件系统变化
2. **索引管理模块**: Whoosh管理文本索引，FAISS管理向量索引
3. **查询处理模块**: 解析用户查询，执行搜索和排序
4. **模型推理模块**: 通过LLaMA.cpp本地推理或API调用
* 5. **历史推荐模块**: 管理搜索历史，生成个性化推荐
* 6. **文档解析模块**: 处理多种格式文档的内容提取

## 4. 功能需求

### 4.1 核心搜索功能
1. **快速文件搜索**
   - 文件名、路径和扩展名搜索
   - 实时索引更新
   - 通配符和正则表达式支持

2. **内容全文搜索**
   - 多格式文档内容提取和索引
   - 关键词高亮显示
   - 搜索结果相关性排序

3. **高级过滤**
   - 文件类型过滤（文档、图片、代码等）
   - 日期范围过滤
   - 文件大小过滤
   - 组合过滤条件

### 4.2 智能功能
1. **搜索历史**
   - 自动记录用户搜索查询
   - 历史搜索快速访问
   - 搜索频率统计和可视化

2. **个性化推荐**
   - 基于搜索历史的文件推荐
   - 相关搜索词建议
   - 热门文件推荐

3. **智能问答**
   - 基于文档内容的自然语言问答
   - 答案来源引用和定位
   - 支持后续追问和上下文保持

### 4.3 文件处理
1. **多格式支持**
   - 文档: PDF, DOC, DOCX, TXT, MD, RTF
   - 电子表格: XLS, XLSX, CSV
   - 演示文稿: PPT, PPTX
   - 图像文件: JPG, PNG (基本元数据)
   - 代码文件: 常见编程语言文件

2. **内容提取**
   - 文本内容提取和清理
   - 元数据提取（作者、日期等）
   - 保持文档结构信息

## 5. 性能指标

### 5.1 响应时间要求
- 关键词搜索响应时间: < 1秒
- 语义搜索响应时间: < 3秒
- 问答生成响应时间: < 5秒
- 初始索引建立时间: < 10分钟(10万文档)

### 5.2 资源使用限制
- 内存使用: < 8GB (包含模型加载)
- 磁盘空间: < 10GB (用于索引和模型存储)
- CPU使用率: < 70% (索引时)

## 6. 用户界面设计

### 6.1 主界面布局
1. **搜索栏区域**
   - 主搜索输入框
   - 快速过滤按钮
   - 高级搜索开关

2. **结果展示区域**
   - 文件列表视图
   - 结果统计信息
   - 排序和视图选项

3. **侧边栏区域**
   - 搜索历史面板
   - 推荐文件列表
   - 过滤器选项

4. **状态栏区域**
   - 索引状态指示
   - 模型状态指示
   - 系统资源使用情况

### 6.2 高级过滤界面
- 直观的过滤条件选择器
- 多条件组合筛选面板
- 实时过滤结果预览

## 7. 部署与运行环境

### 7.1 支持平台
- Windows 10/11 (主要开发平台)
- macOS 10.15+ (次要支持)
- Linux (Ubuntu 18.04+, 通过WSL支持)

### 7.2 硬件要求
**最低配置**
- CPU: 4核心处理器
- 内存: 16GB RAM
- 存储: 256GB SSD
- 显卡: 集成显卡(CPU模式运行)

**推荐配置**
- CPU: 8核心处理器
- 内存: 32GB RAM
- 存储: 512GB NVMe SSD
- 显卡: NVIDIA RTX 3060+(8GB VRAM)

## 8. 项目里程碑

### 8.1 第一阶段: 基础框架 (2周)
- 项目环境搭建和依赖配置
- 基本文件监控和索引功能
- Whoosh关键词搜索实现

### 8.2 第二阶段: 核心功能 (3周)
- 多格式文档解析支持
- FAISS向量索引和语义搜索
- 基本PyQt用户界面

### 8.3 第三阶段: 智能集成 (3周)
- LLaMA.cpp本地模型集成
- 智能问答功能实现
- 搜索历史记录功能

### 8.4 第四阶段: 高级功能 (2周)
- 个性化推荐系统
- 高级过滤界面
- 性能优化和缓存机制

### 8.5 第五阶段: 测试优化 (2周)
- 全面功能测试
- 性能测试和优化
- 用户文档编写

## 9. 风险评估与应对

### 9.1 技术风险
1. **模型性能不足**
   - 应对: 提供模型量化选项和API备用方案

2. **跨平台兼容性问题**
   - 应对: 优先保证Windows兼容性，逐步扩展

3. **大规模文件索引效率**
   - 应对: 实现增量索引和分批处理

### 9.2 资源风险
1. **硬件要求较高**
   - 应对: 提供多级别配置选项和性能调优

2. **模型文件较大**
   - 应对: 提供模型下载工具和进度管理

## 10. 交付物清单

### 10.1 软件交付物
- 可执行应用程序安装包
- 配置文件和默认设置
- 模型下载和管理脚本

### 10.2 文档交付物
- 用户手册和操作指南
- 技术设计文档
- API接口文档(如果提供API)
- 部署和安装指南

### 10.3 测试交付物
- 测试计划和测试用例
- 性能测试报告
- 用户验收测试报告

## 附录: 技术决策理由

### Python选择理由
- 丰富的生态系统和库支持
- 快速开发和原型验证
- 强大的科学计算和AI库支持
- 跨平台兼容性良好

### PyQt选择理由
- 原生外观和性能
- 跨平台支持
- 丰富的UI组件库
- 与Python集成良好

### LLaMA.cpp选择理由
- 本地推理，隐私保护
- 硬件要求相对较低
- 活跃的社区支持
- 模型量化选项丰富

### Whoosh和FAISS选择理由
- Whoosh纯Python实现，易于集成
- FAISS高效的向量相似性搜索
- 两者结合提供关键词+语义搜索能力
- 适合中等规模数据集

本需求文档明确了智能文件检索与问答系统的技术栈和功能要求，为后续开发和测试提供了明确指导。项目采用Python作为主要开发语言，结合PyQt实现用户界面，使用LLaMA.cpp进行本地模型推理，同时保留在线API作为备选方案，确保项目在本科毕业设计范围内可完成。